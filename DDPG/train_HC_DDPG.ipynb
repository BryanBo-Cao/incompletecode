{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gA8FHJqiMgOo"
   },
   "source": [
    "## **OpenAI Gym, PyBullet and PyBulletGym Installation**\n",
    "[Click here to see Gym documentaion](https://gym.openai.com/docs/)\n",
    "\n",
    "[Click here to see PyBullet documentaion](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA)\n",
    "\n",
    "[Click here to see PyBulletGym page](https://github.com/benelot/pybullet-gym)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1LashLPkqYx"
   },
   "source": [
    "Note that this assignment was done in a remote server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nipCFbQpMu8h"
   },
   "source": [
    "**Before we start, first update the apt-get tool in the given machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhNxM6-MVJDV",
    "outputId": "f3ade08f-57a9-4b57-91cb-15b17a921d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zViYHb_7C2fi"
   },
   "outputs": [],
   "source": [
    "# !apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6x2LqNwbMFww"
   },
   "source": [
    "Most of the requirements of python packages are already fulfilled on Colab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EY1Z1npZC5fo"
   },
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !apt-get install python-opengl -y\n",
    "# !apt install xvfb -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fH-G48ZaSlFO"
   },
   "source": [
    "For rendering environment, you can use pyvirtualdisplay. So fulfill that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD8I53YpC92T"
   },
   "outputs": [],
   "source": [
    "# !pip install pyvirtualdisplay\n",
    "# !pip install piglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-z6FT2NYzXE"
   },
   "outputs": [],
   "source": [
    "# !pip install pybullet==2.5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHwqCuAVDrn9"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/benelot/pybullet-gym.git # should already be there in my Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXc2n4nNaeU6"
   },
   "source": [
    "## **Update the source code**\n",
    "In pybulletgym/envs/mujoco/envs/pendulum/inverted_pendulum_env.py, line 32, change\n",
    "\n",
    "done = not np.isfinite(state).all() or np.abs(state[1]) > .2\n",
    "\n",
    "to\n",
    "\n",
    "done = abs(state[0][0]) > 2.4 or abs(state[0][1]) > 0.27\n",
    "\n",
    "**Restart runtime and run the following cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GVojhyza7Nz"
   },
   "outputs": [],
   "source": [
    "# cd /content/pybullet-gym/ # use the address below instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uSrlp8xatq_"
   },
   "outputs": [],
   "source": [
    "# cd './pybullet-gym'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gEJuj5LDxL0"
   },
   "outputs": [],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZVlna0dTh3N"
   },
   "source": [
    "Import everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuMjm4mjC_9T"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "\n",
    "import pybulletgym  # register PyBullet enviroments with open ai gym\n",
    "import pybullet\n",
    "import pybullet_data\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Colab comes with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "import statistics\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jaIPPhAVJHA"
   },
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# All parameters\n",
    "# ---------------\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        # Paramaters for this experiment\n",
    "        self.exp_id = 'htc_actor_critic_pytorch_1'\n",
    "        self.env_id = 'HalfCheetahMuJoCoEnv-v0' # 'HalfCheetahMuJoCoEnv-v0' # 'Pendulum-v0' # 'InvertedPendulumMuJoCoEnv-v0' #\n",
    "        self.server_path = '/home/bryanbc/Repos/rl/'\n",
    "        self.hw = 'hw02'\n",
    "        self.video_path = self.server_path + self.hw + '/' + self.exp_id + '/video/'\n",
    "        self.mp4list_path = self.video_path + '*.mp4'\n",
    "        \n",
    "        self.save_model_episode_interval = 50\n",
    "        self.saved_models_path = '/ssd/bryanbc/saved_models/' + self.hw + '/' + self.exp_id\n",
    "        self.train_start_episode = 0\n",
    "        \n",
    "        # Parameters for models\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 128\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.gamma = 0.99\n",
    "        self.tau   =  0.001 # 0.125\n",
    "        self.max_buffer = 1000000\n",
    "        self.dropout_rate = 0.5\n",
    "        self.sigma = 0.01\n",
    "        \n",
    "        # Parameters for enviroment\n",
    "        self.max_episodes = 5000\n",
    "        self.max_steps = 1000\n",
    "        self.EPS = 0.003\n",
    "        \n",
    "        self.log_path = '/ssd/bryanbc/data/logs/hw/' + self.hw + '/' + self.exp_id + '_'\n",
    "        \n",
    "PARAMS = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybPVfv4RVJHP"
   },
   "outputs": [],
   "source": [
    "# Open log files\n",
    "log_file = open((PARAMS.log_path + 'episode_reward.log'), 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGXHAGHRTJjd"
   },
   "source": [
    "To activate virtual display, we need to run a script once for training an agent, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "50In5ybcFUMm",
    "outputId": "55ac7443-2389-471c-fc14-9d9d8187d888"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':4797'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':4797'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzcbS-GyTUKG"
   },
   "source": [
    "The following code creates a virtual display to draw game images on. If you are running locally, just ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hwrBwc_TSfA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "310x8KjeDF1n"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "# mp4list_path_colab = '/content/gdrive/My Drive/video/*.mp4'\n",
    "def show_video():\n",
    "    mp4list = glob.glob(PARAMS.mp4list_path) # glob.glob('/content/video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "# video_path_colab = '/content/gdrive/My Drive/video/'\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, PARAMS.video_path, force=True) # Monitor(env, '/content/video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWv_fGVXVJJV"
   },
   "source": [
    "# **InvertedPendulumMuJoCoEnv-v0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MP3VCHtWVJJW",
    "outputId": "ff276ff2-3020-4b45-cb13-c132f2e0981d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/pybullet_envs/bullet\n",
      "WalkerBase::__init__\n",
      "s_dim: 17  PARAMS.env.observation_space.shape:  (17,)\n",
      "a_dim:  6  PARAMS.env.action_space.shape:  (6,)\n",
      "PARAMS.env.action_space.high:  [1. 1. 1. 1. 1. 1.]\n",
      "PARAMS.env.action_space.low:  [-1. -1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Create Environment\n",
    "PARAMS.env = gym.make(PARAMS.env_id)\n",
    "PARAMS.env = wrap_env(PARAMS.env)\n",
    "\n",
    "s_dim = PARAMS.env.observation_space.shape[0]\n",
    "a_dim = PARAMS.env.action_space.shape[0]\n",
    "\n",
    "print(\"s_dim:\", s_dim, \" PARAMS.env.observation_space.shape: \", PARAMS.env.observation_space.shape)\n",
    "print(\"a_dim: \", a_dim, \" PARAMS.env.action_space.shape: \", PARAMS.env.action_space.shape)\n",
    "print(\"PARAMS.env.action_space.high: \", PARAMS.env.action_space.high)\n",
    "print(\"PARAMS.env.action_space.low: \", PARAMS.env.action_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxGb76TexllA"
   },
   "source": [
    "# **RL Algorithms**\n",
    "Since the action space is continuous, I use DDPG(Actor-Critic on continuous actions) https://arxiv.org/pdf/1509.02971.pdf. Note that $logπ_{θ}(a|s)$ and the advantage term are not in the code because they are used to approximate the policy gradient $∇_{θ}J(θ)$ with computationally efficiency, while DDPG uses a different way to compute $∇_{θ}J(θ)$ as shown in the report and paper.\n",
    "\n",
    "My code is based on this tutorial: https://github.com/vy007vikas/PyTorch-ActorCriticRL and https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html. Note that action value ranges is [-1, 1] in this hw, so 'tanh' is used as the activation in the last layer to be in the same range as the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXEiW6u6VJJc"
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT7RdyT1VJJd"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------- \n",
    "# OrnsteinUhlenbeckActionNoise the simulate the dynamics noise in the physical world\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\n",
    "    def __init__(self, a_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "        self.a_dim = a_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.X = np.ones(self.a_dim) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones(self.a_dim) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.X)\n",
    "        dx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "        self.X = self.X + dx\n",
    "        return self.X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKSewvd3VJJf"
   },
   "source": [
    "# ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U7RjyvDtFkZI"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ReplayBuffer\n",
    "'''\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.max_size = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        '''\n",
    "        Sample a random batch from the replay buffer.\n",
    "            input:\n",
    "                count: batch size\n",
    "            return:\n",
    "                batch (numpy array)\n",
    "        '''\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        \n",
    "        # sample a batch of transitions from replay buffer\n",
    "        batch = random.sample(self.buffer, count)\n",
    "\n",
    "        s_arr = np.float32([arr[0] for arr in batch])\n",
    "        a_arr = np.float32([arr[1] for arr in batch])\n",
    "        r_arr = np.float32([arr[2] for arr in batch])\n",
    "        s1_arr = np.float32([arr[3] for arr in batch])\n",
    "\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def remember(self, s, a, r, s1):\n",
    "        '''\n",
    "        Append a transition in the replay buffer.\n",
    "            input:\n",
    "                transition (s, a, r, s1)\n",
    "        '''\n",
    "        transition = (s, a, r, s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.max_size:\n",
    "            self.len = self.max_size\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xP9QeiEzVJJi"
   },
   "source": [
    "# Actor Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB199JmyVJJi"
   },
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "'''\n",
    "Actor\n",
    "'''\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        '''\n",
    "        input:\n",
    "            s_dim: state dimension (int)\n",
    "            a_dim: output action dimension (int)\n",
    "        '''\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(s_dim, 256)\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc3.weight.data = fanin_init(self.fc3.weight.data.size())\n",
    "\n",
    "        self.fc4 = nn.Linear(64, a_dim)\n",
    "        self.fc4.weight.data.uniform_(-PARAMS.EPS,PARAMS.EPS)\n",
    "\n",
    "    def forward(self, s):\n",
    "        '''\n",
    "        Return policy function Pi(s) obtained from actor network.\n",
    "        '''\n",
    "        x = F.relu(self.fc1(s))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        a = F.tanh(self.fc4(x)) # tanh to match the action space [-1, 1]\n",
    "        return a\n",
    "\n",
    "\n",
    "'''\n",
    "Critic\n",
    "'''\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        '''\n",
    "        input:\n",
    "            s_dim: state dimension (int)\n",
    "            a_dim: output action dimension (int)\n",
    "        '''\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        self.fc_s1 = nn.Linear(s_dim, 256)\n",
    "        self.fc_s1.weight.data = fanin_init(self.fc_s1.weight.data.size())\n",
    "        self.fc_s2 = nn.Linear(256, 128)\n",
    "        self.fc_s2.weight.data = fanin_init(self.fc_s2.weight.data.size())\n",
    "\n",
    "        self.fc_a1 = nn.Linear(a_dim, 128)\n",
    "        self.fc_a1.weight.data = fanin_init(self.fc_a1.weight.data.size())\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.fc3.weight.data.uniform_(-PARAMS.EPS, PARAMS.EPS)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        s1 = F.relu(self.fc_s1(s))\n",
    "        s2 = F.relu(self.fc_s2(s1))\n",
    "        a1 = F.relu(self.fc_a1(a))\n",
    "        x = torch.cat((s2, a1), dim=1)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "'''\n",
    "Actor-critic\n",
    "'''\n",
    "class ActorCritic:\n",
    "\n",
    "    def __init__(self, s_dim, a_dim, replay_buffer):\n",
    "        '''\n",
    "        input:\n",
    "            s_dim: state dimension (int)\n",
    "            a_dim: output action dimension (int)\n",
    "            replay_buffer: replay_buffer object\n",
    "        '''\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(self.a_dim)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        #  Create network and target network instance for Actor\n",
    "        # ------------------------------------------------------\n",
    "        self.actor = Actor(self.s_dim, self.a_dim)\n",
    "        self.target_actor = Actor(self.s_dim, self.a_dim)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), PARAMS.learning_rate)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        #  Create network and target network instance for Critic\n",
    "        # ------------------------------------------------------\n",
    "        self.critic = Critic(self.s_dim, self.a_dim)\n",
    "        self.target_critic = Critic(self.s_dim, self.a_dim)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), PARAMS.learning_rate)\n",
    "\n",
    "        self.hard_update(self.target_actor, self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "        \n",
    "    def select_action(self, s, a_type=\"exploitation\"):\n",
    "        '''\n",
    "        input:\n",
    "            s: state (Numpy array)\n",
    "            a: sampled action (Numpy array)\n",
    "        '''\n",
    "        if a_type == \"exploration\":\n",
    "            '''\n",
    "            Return an action from actor added with exploration noise.\n",
    "            '''\n",
    "            s = Variable(torch.from_numpy(s))\n",
    "            a = self.actor.forward(s).detach()\n",
    "            new_a = a.data.numpy() + (self.noise.sample())\n",
    "            return np.clip(new_a, PARAMS.env.action_space.low, PARAMS.env.action_space.high)\n",
    "        else:\n",
    "            '''\n",
    "            a_type=\"exploitation\"\n",
    "            Return an action from target actor added with exploration noise\n",
    "            '''\n",
    "            s = Variable(torch.from_numpy(s))\n",
    "            a = self.target_actor.forward(s).detach()\n",
    "        return np.clip(a.data.numpy(), PARAMS.env.action_space.low, PARAMS.env.action_space.high)\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Sample a random batch from replay memory and train the actor-critic model.\n",
    "        '''\n",
    "        s1, a1, r1, s2 = self.replay_buffer.sample(PARAMS.batch_size)\n",
    "\n",
    "        # Convert numpy variables to Pytorch ones.\n",
    "        s1 = Variable(torch.from_numpy(s1))\n",
    "        a1 = Variable(torch.from_numpy(a1))\n",
    "        r1 = Variable(torch.from_numpy(r1))\n",
    "        s2 = Variable(torch.from_numpy(s2))\n",
    "\n",
    "        # -------------\n",
    "        # Train critic\n",
    "        # -------------\n",
    "        # Use target actor exploitation policy here for loss evaluation\n",
    "        a2 = self.target_actor.forward(s2).detach()\n",
    "        next_v = torch.squeeze(self.target_critic.forward(s2, a2).detach())\n",
    "        exp_y = r1 + PARAMS.gamma * next_v\n",
    "        pred_y = torch.squeeze(self.critic.forward(s1, a1))\n",
    "        \n",
    "        # compute critic loss, and update the critic\n",
    "        loss_critic = F.smooth_l1_loss(pred_y, exp_y)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ------------\n",
    "        # Train actor\n",
    "        # ------------\n",
    "        pred_a1 = self.actor.forward(s1)\n",
    "        # \"Using gradient ascent, we can move θ toward the direction \n",
    "        # suggested by the gradient ∇θJ(θ) to find the best θ for πθ \n",
    "        # that produces the highest return.\" \n",
    "        #   https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "        # Thus the actor loss is negated.\n",
    "        loss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # soft update -- copy weighted parameters instead of directly\n",
    "        self.soft_update(self.target_actor, self.actor, PARAMS.tau)\n",
    "        self.soft_update(self.target_critic, self.critic, PARAMS.tau)\n",
    "\n",
    "    def save_models(self, episode_i):\n",
    "        torch.save(self.target_actor.state_dict(), \\\n",
    "                   '%s/actor_episode_%d.pt' % \n",
    "                        (PARAMS.saved_models_path, episode_i))\n",
    "        torch.save(self.target_critic.state_dict(), \\\n",
    "                   '%s/critic_episode_%d.pt' % \n",
    "                        (PARAMS.saved_models_path, episode_i))\n",
    "        print('Models %s/actor_episode_%d.pt and %s/critic_episode_%d.pt saved successfully' % \n",
    "                        (PARAMS.saved_models_path, episode_i,\n",
    "                         PARAMS.saved_models_path, episode_i))\n",
    "\n",
    "    def load_models(self, episode_i):\n",
    "        self.actor.load_state_dict(torch.load('%s/actor_episode_%d.pt' % \n",
    "                        (PARAMS.saved_models_path, episode_i)))\n",
    "        self.critic.load_state_dict(torch.load('%s/critic_episode_%d.pt' % \n",
    "                        (PARAMS.saved_models_path, episode_i)))\n",
    "        self.hard_update(self.target_actor, self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "        print('Models %s/actor_episode_%d.pt and %s/critic_episode_%d.pt loaded successfully' % \n",
    "                        (PARAMS.saved_models_path, episode_i,\n",
    "                         PARAMS.saved_models_path, episode_i))\n",
    "        \n",
    "    def soft_update(self, target, source, tau):\n",
    "        '''\n",
    "        Copies the parameters from source network (x) to target network (y) using the below update\n",
    "        y = TAU*x + (1 - TAU)*y\n",
    "        input:\n",
    "            target: Target network (PyTorch)\n",
    "            source: Source network (PyTorch)\n",
    "        '''\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - tau) + param.data * tau\n",
    "            )\n",
    "\n",
    "    def hard_update(self, target, source):\n",
    "        '''\n",
    "        Copies the parameters from source network to target network\n",
    "        input:\n",
    "            target: Target network (PyTorch)\n",
    "             source: Source network (PyTorch)\n",
    "        '''\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sp3ZslpRVJJl"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "colab_type": "code",
    "id": "3l_Q9c24GQ1M",
    "outputId": "301c1269-e4b3-4739-8e49-3b5e3fc4559e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_0.pt or /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_0.pt does not exist. Will train the model from episode 0.\n",
      "options= \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:161: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  0 episode reward:  -407.7514723315412  Episode finished after 1000 timesteps\n",
      "Models /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_0.pt and /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/critic_episode_0.pt saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  1 episode reward:  -114.077880891529  Episode finished after 1000 timesteps\n",
      "episode_i:  2 episode reward:  -99.13269838623003  Episode finished after 1000 timesteps\n",
      "episode_i:  3 episode reward:  -61.19484414589196  Episode finished after 1000 timesteps\n",
      "episode_i:  4 episode reward:  -79.18515017953045  Episode finished after 1000 timesteps\n",
      "episode_i:  5 episode reward:  -85.99512327213516  Episode finished after 1000 timesteps\n",
      "episode_i:  6 episode reward:  -99.54744067070662  Episode finished after 1000 timesteps\n",
      "episode_i:  7 episode reward:  -64.45424406464234  Episode finished after 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  8 episode reward:  -50.23435435130421  Episode finished after 1000 timesteps\n",
      "episode_i:  9 episode reward:  -80.00551991182924  Episode finished after 1000 timesteps\n",
      "episode_i:  10 episode reward:  -13.574291815624855  mean reward:  -74.74015476894239  Episode finished after 1000 timesteps\n",
      "episode_i:  11 episode reward:  -47.714130513365674  mean reward:  -68.10377973112605  Episode finished after 1000 timesteps\n",
      "episode_i:  12 episode reward:  -44.47870815439793  mean reward:  -62.63838070794284  Episode finished after 1000 timesteps\n",
      "episode_i:  13 episode reward:  33.846028355389606  mean reward:  -53.13429345781468  Episode finished after 1000 timesteps\n",
      "episode_i:  14 episode reward:  47.42494712752842  mean reward:  -40.4732837271088  Episode finished after 1000 timesteps\n",
      "episode_i:  15 episode reward:  147.1959724773489  mean reward:  -17.154174152160397  Episode finished after 1000 timesteps\n",
      "episode_i:  16 episode reward:  227.31112074394753  mean reward:  15.531681989305019  Episode finished after 1000 timesteps\n",
      "episode_i:  17 episode reward:  152.3698839144633  mean reward:  37.214094787215586  Episode finished after 1000 timesteps\n",
      "episode_i:  18 episode reward:  55.358657474652894  mean reward:  47.7733959698113  Episode finished after 1000 timesteps\n",
      "episode_i:  19 episode reward:  189.47076496848302  mean reward:  74.72102445784253  Episode finished after 1000 timesteps\n",
      "episode_i:  20 episode reward:  191.21374680638107  mean reward:  95.19982832004311  Episode finished after 1000 timesteps\n",
      "episode_i:  21 episode reward:  47.37882281152445  mean reward:  104.70912365253211  Episode finished after 1000 timesteps\n",
      "episode_i:  22 episode reward:  -175.4009655935577  mean reward:  91.61689790861615  Episode finished after 1000 timesteps\n",
      "episode_i:  23 episode reward:  136.92819893055056  mean reward:  101.92511496613224  Episode finished after 1000 timesteps\n",
      "episode_i:  24 episode reward:  -55.648471749145465  mean reward:  91.61777307846485  Episode finished after 1000 timesteps\n",
      "episode_i:  25 episode reward:  25.57240124678686  mean reward:  79.45541595540865  Episode finished after 1000 timesteps\n",
      "episode_i:  26 episode reward:  328.36530070720124  mean reward:  89.56083395173403  Episode finished after 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  27 episode reward:  355.15055434455104  mean reward:  109.83890099474281  Episode finished after 1000 timesteps\n",
      "episode_i:  28 episode reward:  388.29919702142485  mean reward:  143.13295494942003  Episode finished after 1000 timesteps\n",
      "episode_i:  29 episode reward:  406.46636940936065  mean reward:  164.83251539350778  Episode finished after 1000 timesteps\n",
      "episode_i:  30 episode reward:  440.03025317485594  mean reward:  189.71416603035522  Episode finished after 1000 timesteps\n",
      "episode_i:  31 episode reward:  432.0529999099895  mean reward:  228.18158374020172  Episode finished after 1000 timesteps\n",
      "episode_i:  32 episode reward:  497.3664760562754  mean reward:  295.458327905185  Episode finished after 1000 timesteps\n",
      "episode_i:  33 episode reward:  355.9516943563558  mean reward:  317.36067744776557  Episode finished after 1000 timesteps\n",
      "episode_i:  34 episode reward:  443.37562911346373  mean reward:  367.2630875340265  Episode finished after 1000 timesteps\n",
      "episode_i:  35 episode reward:  507.9782657446117  mean reward:  415.50367398380894  Episode finished after 1000 timesteps\n",
      "episode_i:  36 episode reward:  559.3749350323249  mean reward:  438.6046374163214  Episode finished after 1000 timesteps\n",
      "episode_i:  37 episode reward:  676.5101143520642  mean reward:  470.7405934170727  Episode finished after 1000 timesteps\n",
      "episode_i:  38 episode reward:  584.2469924827402  mean reward:  490.3353729632042  Episode finished after 1000 timesteps\n",
      "episode_i:  39 episode reward:  570.8642180399273  mean reward:  506.77515782626085  Episode finished after 1000 timesteps\n",
      "episode_i:  40 episode reward:  585.1318420643586  mean reward:  521.2853167152111  Episode finished after 1000 timesteps\n",
      "episode_i:  41 episode reward:  621.2593305149707  mean reward:  540.2059497757093  Episode finished after 1000 timesteps\n",
      "episode_i:  42 episode reward:  628.509239197427  mean reward:  553.3202260898245  Episode finished after 1000 timesteps\n",
      "episode_i:  43 episode reward:  733.7230437522159  mean reward:  591.0973610294104  Episode finished after 1000 timesteps\n",
      "episode_i:  44 episode reward:  708.4631338507681  mean reward:  617.6061115031409  Episode finished after 1000 timesteps\n",
      "episode_i:  45 episode reward:  754.3363128066565  mean reward:  642.2419162093454  Episode finished after 1000 timesteps\n",
      "episode_i:  46 episode reward:  745.3476769022391  mean reward:  660.8391903963368  Episode finished after 1000 timesteps\n",
      "episode_i:  47 episode reward:  820.2201280621834  mean reward:  675.2101917673486  Episode finished after 1000 timesteps\n",
      "episode_i:  48 episode reward:  -393.50915876656944  mean reward:  577.4345766424177  Episode finished after 1000 timesteps\n",
      "episode_i:  49 episode reward:  891.5333309453307  mean reward:  609.5014879329581  Episode finished after 1000 timesteps\n",
      "episode_i:  50 episode reward:  829.3655203601384  mean reward:  633.9248557625361  Episode finished after 1000 timesteps\n",
      "Models /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_50.pt and /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/critic_episode_50.pt saved successfully\n",
      "episode_i:  51 episode reward:  606.8450893707051  mean reward:  632.4834316481094  Episode finished after 1000 timesteps\n",
      "episode_i:  52 episode reward:  -153.36882396207153  mean reward:  554.2956253321597  Episode finished after 1000 timesteps\n",
      "episode_i:  53 episode reward:  898.365887659007  mean reward:  570.7599097228388  Episode finished after 1000 timesteps\n",
      "episode_i:  54 episode reward:  879.6282163613873  mean reward:  587.8764179739007  Episode finished after 1000 timesteps\n",
      "episode_i:  55 episode reward:  773.7760683520236  mean reward:  589.8203935284374  Episode finished after 1000 timesteps\n",
      "episode_i:  56 episode reward:  869.6575749285118  mean reward:  602.2513833310647  Episode finished after 1000 timesteps\n",
      "episode_i:  57 episode reward:  855.0093751796453  mean reward:  605.7303080428109  Episode finished after 1000 timesteps\n",
      "episode_i:  58 episode reward:  867.7970363186555  mean reward:  731.8609275513334  Episode finished after 1000 timesteps\n",
      "episode_i:  59 episode reward:  879.882130450542  mean reward:  730.6958075018545  Episode finished after 1000 timesteps\n",
      "episode_i:  60 episode reward:  669.9326058943362  mean reward:  714.7525160552743  Episode finished after 1000 timesteps\n",
      "episode_i:  61 episode reward:  933.1669429380072  mean reward:  747.3847014120045  Episode finished after 1000 timesteps\n",
      "episode_i:  62 episode reward:  909.6069365619558  mean reward:  853.6822774644072  Episode finished after 1000 timesteps\n",
      "episode_i:  63 episode reward:  748.0821405610285  mean reward:  838.6539027546094  Episode finished after 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  64 episode reward:  924.6850624764329  mean reward:  843.1595873661139  Episode finished after 1000 timesteps\n",
      "episode_i:  65 episode reward:  758.4142394981261  mean reward:  841.623404480724  Episode finished after 1000 timesteps\n",
      "episode_i:  66 episode reward:  983.480807088133  mean reward:  853.0057276966863  Episode finished after 1000 timesteps\n",
      "episode_i:  67 episode reward:  892.0800104470626  mean reward:  856.712791223428  Episode finished after 1000 timesteps\n",
      "episode_i:  68 episode reward:  940.8284767821556  mean reward:  864.015935269778  Episode finished after 1000 timesteps\n",
      "episode_i:  69 episode reward:  1088.5984083722465  mean reward:  884.8875630619484  Episode finished after 1000 timesteps\n",
      "episode_i:  70 episode reward:  890.1231936842383  mean reward:  906.9066218409387  Episode finished after 1000 timesteps\n",
      "episode_i:  71 episode reward:  994.3872735958681  mean reward:  913.0286549067248  Episode finished after 1000 timesteps\n",
      "episode_i:  72 episode reward:  1092.817319016217  mean reward:  931.349693152151  Episode finished after 1000 timesteps\n",
      "episode_i:  73 episode reward:  1078.9230136020713  mean reward:  964.4337804562552  Episode finished after 1000 timesteps\n",
      "episode_i:  74 episode reward:  938.0774459293885  mean reward:  965.7730188015508  Episode finished after 1000 timesteps\n",
      "episode_i:  75 episode reward:  1064.5067432092421  mean reward:  996.3822691726624  Episode finished after 1000 timesteps\n",
      "episode_i:  76 episode reward:  -777.3114247848795  mean reward:  820.3030459853611  Episode finished after 1000 timesteps\n",
      "episode_i:  77 episode reward:  947.9602300847837  mean reward:  825.8910679491331  Episode finished after 1000 timesteps\n",
      "episode_i:  78 episode reward:  985.0496164621952  mean reward:  830.3131819171371  Episode finished after 1000 timesteps\n",
      "episode_i:  79 episode reward:  1033.5028490410384  mean reward:  824.8036259840162  Episode finished after 1000 timesteps\n",
      "episode_i:  80 episode reward:  1066.528336765815  mean reward:  842.4441402921741  Episode finished after 1000 timesteps\n",
      "episode_i:  81 episode reward:  1034.61691587253  mean reward:  846.4671045198402  Episode finished after 1000 timesteps\n",
      "episode_i:  82 episode reward:  955.1448666721807  mean reward:  832.6998592854364  Episode finished after 1000 timesteps\n",
      "episode_i:  83 episode reward:  1002.79083387963  mean reward:  825.0866413131923  Episode finished after 1000 timesteps\n",
      "episode_i:  84 episode reward:  1063.881775994761  mean reward:  837.6670743197297  Episode finished after 1000 timesteps\n",
      "episode_i:  85 episode reward:  999.2962696473838  mean reward:  831.1460269635438  Episode finished after 1000 timesteps\n",
      "episode_i:  86 episode reward:  986.2640622733186  mean reward:  1007.5035756693636  Episode finished after 1000 timesteps\n",
      "episode_i:  87 episode reward:  1093.8898976569549  mean reward:  1022.0965424265808  Episode finished after 1000 timesteps\n",
      "episode_i:  88 episode reward:  954.7228082851483  mean reward:  1019.0638616088761  Episode finished after 1000 timesteps\n",
      "episode_i:  89 episode reward:  964.910566572092  mean reward:  1012.2046333619816  Episode finished after 1000 timesteps\n",
      "episode_i:  90 episode reward:  1010.7402502880984  mean reward:  1006.6258247142098  Episode finished after 1000 timesteps\n",
      "episode_i:  91 episode reward:  1025.4074586118766  mean reward:  1005.7048789881444  Episode finished after 1000 timesteps\n",
      "episode_i:  92 episode reward:  1052.550600720837  mean reward:  1015.4454523930102  Episode finished after 1000 timesteps\n",
      "episode_i:  93 episode reward:  1152.4218394355228  mean reward:  1030.4085529485994  Episode finished after 1000 timesteps\n",
      "episode_i:  94 episode reward:  994.4816580323545  mean reward:  1023.4685411523587  Episode finished after 1000 timesteps\n",
      "episode_i:  95 episode reward:  1123.7375445726047  mean reward:  1035.9126686448808  Episode finished after 1000 timesteps\n",
      "episode_i:  96 episode reward:  1105.1383719928801  mean reward:  1047.800099616837  Episode finished after 1000 timesteps\n",
      "episode_i:  97 episode reward:  1116.8750891962652  mean reward:  1050.098618770768  Episode finished after 1000 timesteps\n",
      "episode_i:  98 episode reward:  1094.8026591525033  mean reward:  1064.1066038575034  Episode finished after 1000 timesteps\n",
      "episode_i:  99 episode reward:  1115.080409359804  mean reward:  1079.1235881362745  Episode finished after 1000 timesteps\n",
      "episode_i:  100 episode reward:  1168.2793635341302  mean reward:  1094.8774994608777  Episode finished after 1000 timesteps\n",
      "Models /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_100.pt and /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/critic_episode_100.pt saved successfully\n",
      "episode_i:  101 episode reward:  1131.8544902713206  mean reward:  1105.522202626822  Episode finished after 1000 timesteps\n",
      "episode_i:  102 episode reward:  1086.2881854109517  mean reward:  1108.8959610958336  Episode finished after 1000 timesteps\n",
      "episode_i:  103 episode reward:  1185.575841880213  mean reward:  1112.2113613403026  Episode finished after 1000 timesteps\n",
      "episode_i:  104 episode reward:  1062.0935562236903  mean reward:  1118.9725511594363  Episode finished after 1000 timesteps\n",
      "episode_i:  105 episode reward:  1181.7145049118685  mean reward:  1124.7702471933626  Episode finished after 1000 timesteps\n",
      "episode_i:  106 episode reward:  1108.348986465489  mean reward:  1125.0913086406235  Episode finished after 1000 timesteps\n",
      "episode_i:  107 episode reward:  1242.3631901129934  mean reward:  1137.6401187322963  Episode finished after 1000 timesteps\n",
      "episode_i:  108 episode reward:  -273.9390054640288  mean reward:  1000.7659522706432  Episode finished after 1000 timesteps\n",
      "episode_i:  109 episode reward:  1095.396796191902  mean reward:  998.797590953853  Episode finished after 1000 timesteps\n",
      "episode_i:  110 episode reward:  1176.8685514673691  mean reward:  999.656509747177  Episode finished after 1000 timesteps\n",
      "episode_i:  111 episode reward:  1128.6526162751393  mean reward:  999.3363223475587  Episode finished after 1000 timesteps\n",
      "episode_i:  112 episode reward:  1187.6785742389327  mean reward:  1009.475361230357  Episode finished after 1000 timesteps\n",
      "episode_i:  113 episode reward:  1161.115556690789  mean reward:  1007.0293327114147  Episode finished after 1000 timesteps\n",
      "episode_i:  114 episode reward:  1059.7436650295429  mean reward:  1006.7943435919999  Episode finished after 1000 timesteps\n",
      "episode_i:  115 episode reward:  1214.3887486679832  mean reward:  1010.0617679676113  Episode finished after 1000 timesteps\n",
      "episode_i:  116 episode reward:  1095.6284305646502  mean reward:  1008.7897123775276  Episode finished after 1000 timesteps\n",
      "episode_i:  117 episode reward:  1218.7750975299614  mean reward:  1006.4309031192245  Episode finished after 1000 timesteps\n",
      "episode_i:  118 episode reward:  1199.2948935024872  mean reward:  1153.7542930158759  Episode finished after 1000 timesteps\n",
      "episode_i:  119 episode reward:  1163.2562575169861  mean reward:  1160.5402391483844  Episode finished after 1000 timesteps\n",
      "episode_i:  120 episode reward:  1225.344415876081  mean reward:  1165.3878255892553  Episode finished after 1000 timesteps\n",
      "episode_i:  121 episode reward:  1159.4329399015844  mean reward:  1168.4658579519  Episode finished after 1000 timesteps\n",
      "episode_i:  122 episode reward:  1097.5661262682559  mean reward:  1159.4546131548323  Episode finished after 1000 timesteps\n",
      "episode_i:  123 episode reward:  1124.9474255526404  mean reward:  1155.8378000410173  Episode finished after 1000 timesteps\n",
      "episode_i:  124 episode reward:  1072.2425979462962  mean reward:  1157.0876933326927  Episode finished after 1000 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanbc/Apps/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_i:  125 episode reward:  1352.5578059880506  mean reward:  1170.9045990646996  Episode finished after 1000 timesteps\n",
      "episode_i:  126 episode reward:  1157.0741866591943  mean reward:  1177.0491746741538  Episode finished after 1000 timesteps\n",
      "episode_i:  127 episode reward:  1249.811184964978  mean reward:  1180.1527834176554  Episode finished after 1000 timesteps\n",
      "episode_i:  128 episode reward:  1120.1733616495178  mean reward:  1172.2406302323586  Episode finished after 1000 timesteps\n",
      "episode_i:  129 episode reward:  1198.5333853548716  mean reward:  1175.7683430161471  Episode finished after 1000 timesteps\n",
      "episode_i:  130 episode reward:  1154.690649207908  mean reward:  1168.7029663493297  Episode finished after 1000 timesteps\n",
      "episode_i:  131 episode reward:  1244.0418481519077  mean reward:  1177.163857174362  Episode finished after 1000 timesteps\n",
      "episode_i:  132 episode reward:  1133.652363508197  mean reward:  1180.7724808983562  Episode finished after 1000 timesteps\n",
      "episode_i:  133 episode reward:  1098.5005716522464  mean reward:  1178.1277955083167  Episode finished after 1000 timesteps\n",
      "episode_i:  134 episode reward:  1144.6775531329417  mean reward:  1185.3712910269812  Episode finished after 1000 timesteps\n",
      "episode_i:  135 episode reward:  1203.7084159453848  mean reward:  1170.4863520227148  Episode finished after 1000 timesteps\n",
      "episode_i:  136 episode reward:  1158.2977731926892  mean reward:  1170.6087106760642  Episode finished after 1000 timesteps\n",
      "episode_i:  137 episode reward:  1170.105539015366  mean reward:  1162.638146081103  Episode finished after 1000 timesteps\n",
      "episode_i:  138 episode reward:  1112.7767500865373  mean reward:  1161.898484924805  Episode finished after 1000 timesteps\n",
      "episode_i:  139 episode reward:  1232.8269206640089  mean reward:  1165.3278384557188  Episode finished after 1000 timesteps\n",
      "episode_i:  140 episode reward:  1286.2348927773169  mean reward:  1178.4822628126597  Episode finished after 1000 timesteps\n",
      "episode_i:  141 episode reward:  1237.847790133124  mean reward:  1177.8628570107812  Episode finished after 1000 timesteps\n",
      "episode_i:  142 episode reward:  1142.095953666791  mean reward:  1178.7072160266407  Episode finished after 1000 timesteps\n",
      "episode_i:  143 episode reward:  1272.6812372882116  mean reward:  1196.1252825902372  Episode finished after 1000 timesteps\n",
      "episode_i:  144 episode reward:  1302.6900172767894  mean reward:  1211.926529004622  Episode finished after 1000 timesteps\n",
      "episode_i:  145 episode reward:  1138.856745848696  mean reward:  1205.4413619949532  Episode finished after 1000 timesteps\n",
      "episode_i:  146 episode reward:  1157.795270033695  mean reward:  1205.3911116790537  Episode finished after 1000 timesteps\n",
      "episode_i:  147 episode reward:  1246.271095030811  mean reward:  1213.007667280598  Episode finished after 1000 timesteps\n",
      "episode_i:  148 episode reward:  1326.453708612542  mean reward:  1234.3753631331986  Episode finished after 1000 timesteps\n",
      "episode_i:  149 episode reward:  1128.685213797177  mean reward:  1223.9611924465153  Episode finished after 1000 timesteps\n",
      "episode_i:  150 episode reward:  1229.0823984480646  mean reward:  1218.24594301359  Episode finished after 1000 timesteps\n",
      "Models /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/actor_episode_150.pt and /ssd/bryanbc/saved_models/hw02/htc_actor_critic_pytorch_1/critic_episode_150.pt saved successfully\n",
      "episode_i:  151 episode reward:  1217.1061357183871  mean reward:  1216.1717775721165  Episode finished after 1000 timesteps\n",
      "episode_i:  152 episode reward:  -236.23632462391348  mean reward:  1078.3385497430459  Episode finished after 1000 timesteps\n",
      "episode_i:  153 episode reward:  1331.0894750911566  mean reward:  1084.1793735233405  Episode finished after 1000 timesteps\n",
      "episode_i:  154 episode reward:  1241.0762296153819  mean reward:  1078.0179947571996  Episode finished after 1000 timesteps\n",
      "episode_i:  155 episode reward:  1254.7111400296121  mean reward:  1089.6034341752916  Episode finished after 1000 timesteps\n",
      "episode_i:  156 episode reward:  1134.617216533263  mean reward:  1087.2856288252483  Episode finished after 1000 timesteps\n",
      "episode_i:  157 episode reward:  1199.3060876436862  mean reward:  1082.5891280865358  Episode finished after 1000 timesteps\n",
      "episode_i:  158 episode reward:  1206.8094552196228  mean reward:  1070.6247027472439  Episode finished after 1000 timesteps\n",
      "episode_i:  159 episode reward:  1360.2265784203628  mean reward:  1093.7788392095624  Episode finished after 1000 timesteps\n",
      "episode_i:  160 episode reward:  1283.690172882067  mean reward:  1099.2396166529627  Episode finished after 1000 timesteps\n",
      "episode_i:  161 episode reward:  1299.7566991139815  mean reward:  1107.504672992522  Episode finished after 1000 timesteps\n",
      "episode_i:  162 episode reward:  1229.4584095885084  mean reward:  1254.0741464137643  Episode finished after 1000 timesteps\n",
      "episode_i:  163 episode reward:  1355.0884356734011  mean reward:  1256.474042471989  Episode finished after 1000 timesteps\n",
      "episode_i:  164 episode reward:  1190.950566385458  mean reward:  1251.4614761489963  Episode finished after 1000 timesteps\n",
      "episode_i:  165 episode reward:  1180.791657436533  mean reward:  1244.0695278896885  Episode finished after 1000 timesteps\n",
      "episode_i:  166 episode reward:  1348.8639493783724  mean reward:  1265.4942011741994  Episode finished after 1000 timesteps\n",
      "episode_i:  167 episode reward:  1320.0108989616162  mean reward:  1277.5646823059924  Episode finished after 1000 timesteps\n",
      "episode_i:  168 episode reward:  1204.6778534554855  mean reward:  1277.3515221295786  Episode finished after 1000 timesteps\n",
      "episode_i:  169 episode reward:  1206.4428046691075  mean reward:  1261.9731447544532  Episode finished after 1000 timesteps\n",
      "episode_i:  170 episode reward:  1271.0288408096026  mean reward:  1260.7070115472065  Episode finished after 1000 timesteps\n",
      "episode_i:  171 episode reward:  1302.836985503323  mean reward:  1261.0150401861406  Episode finished after 1000 timesteps\n",
      "episode_i:  172 episode reward:  1307.721355977723  mean reward:  1268.8413348250622  Episode finished after 1000 timesteps\n",
      "episode_i:  173 episode reward:  1230.7386103344443  mean reward:  1256.4063522911665  Episode finished after 1000 timesteps\n",
      "episode_i:  174 episode reward:  1379.7434535608122  mean reward:  1275.285641008702  Episode finished after 1000 timesteps\n",
      "episode_i:  175 episode reward:  1281.0806375241823  mean reward:  1285.3145390174668  Episode finished after 1000 timesteps\n",
      "episode_i:  176 episode reward:  1300.7762149831656  mean reward:  1280.5057655779462  Episode finished after 1000 timesteps\n",
      "episode_i:  177 episode reward:  1333.6212265142606  mean reward:  1281.8667983332107  Episode finished after 1000 timesteps\n",
      "episode_i:  178 episode reward:  1370.5079778558643  mean reward:  1298.4498107732486  Episode finished after 1000 timesteps\n",
      "episode_i:  179 episode reward:  1316.2879989452765  mean reward:  1309.4343302008654  Episode finished after 1000 timesteps\n",
      "episode_i:  180 episode reward:  1364.2626079578254  mean reward:  1318.7577069156878  Episode finished after 1000 timesteps\n",
      "episode_i:  181 episode reward:  1405.8991124081122  mean reward:  1329.0639196061666  Episode finished after 1000 timesteps\n",
      "episode_i:  182 episode reward:  1329.9344280058751  mean reward:  1331.2852268089819  Episode finished after 1000 timesteps\n",
      "episode_i:  183 episode reward:  1263.157415985167  mean reward:  1334.5271073740541  Episode finished after 1000 timesteps\n",
      "episode_i:  184 episode reward:  1277.6558911218708  mean reward:  1324.31835113016  Episode finished after 1000 timesteps\n",
      "episode_i:  185 episode reward:  1395.042612640774  mean reward:  1335.714548641819  Episode finished after 1000 timesteps\n",
      "episode_i:  186 episode reward:  1328.1345241900906  mean reward:  1338.4503795625117  Episode finished after 1000 timesteps\n",
      "episode_i:  187 episode reward:  -491.0472203383998  mean reward:  1155.9835348772453  Episode finished after 1000 timesteps\n",
      "episode_i:  188 episode reward:  1378.5119401871887  mean reward:  1156.7839311103778  Episode finished after 1000 timesteps\n",
      "episode_i:  189 episode reward:  1245.1147239195259  mean reward:  1149.6666036078027  Episode finished after 1000 timesteps\n",
      "episode_i:  190 episode reward:  1274.4840234637327  mean reward:  1140.6887451583937  Episode finished after 1000 timesteps\n",
      "episode_i:  191 episode reward:  1207.6632416930124  mean reward:  1120.8651580868836  Episode finished after 1000 timesteps\n",
      "episode_i:  192 episode reward:  1275.370324496959  mean reward:  1115.4087477359922  Episode finished after 1000 timesteps\n",
      "episode_i:  193 episode reward:  1383.8075607396738  mean reward:  1127.4737622114428  Episode finished after 1000 timesteps\n",
      "episode_i:  194 episode reward:  1347.4523319483003  mean reward:  1134.4534062940857  Episode finished after 1000 timesteps\n",
      "episode_i:  195 episode reward:  1434.4956522684145  mean reward:  1138.3987102568497  Episode finished after 1000 timesteps\n",
      "episode_i:  196 episode reward:  1420.8069130681315  mean reward:  1147.665949144654  Episode finished after 1000 timesteps\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "### Training ###\n",
    "################\n",
    "# Let the agent interact with the environment\n",
    "total_steps = 0\n",
    "last_ten_episode_rewards, last_ten_episode_rewards_i = [], 0\n",
    "\n",
    "# the saved model to start training\n",
    "episode_i = PARAMS.train_start_episode\n",
    "\n",
    "# Create an instance of replay buffer.\n",
    "replay_buffer = ReplayBuffer(PARAMS.max_buffer)\n",
    "# Create an instance of ActorCritic model.\n",
    "actor_critic = ActorCritic(s_dim, a_dim, replay_buffer)\n",
    "\n",
    "# -------------\n",
    "# Load weights\n",
    "# -------------\n",
    "os.makedirs(PARAMS.saved_models_path, exist_ok=True)\n",
    "if path.exists('%s/actor_episode_%d.pt' % \n",
    "        (PARAMS.saved_models_path, episode_i)) and \\\n",
    "    path.exists('%s/critic_episode_%d.pt' % \n",
    "        (PARAMS.saved_models_path, episode_i)):\n",
    "    actor_critic.load_models(episode_i)\n",
    "else:\n",
    "    print('%s/actor_episode_%d.pt or %s/actor_episode_%d.pt does not exist. Will train the model from episode 0.' % \n",
    "                        (PARAMS.saved_models_path, episode_i,\n",
    "                         PARAMS.saved_models_path, episode_i))\n",
    "    episode_i = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Iterate through all episodes\n",
    "# -----------------------------\n",
    "while episode_i < PARAMS.max_episodes:\n",
    "    obs = PARAMS.env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Iterate through all steps\n",
    "    for t in range(PARAMS.max_steps):\n",
    "        PARAMS.env.render()\n",
    "        \n",
    "        # s: current state\n",
    "        s = np.float32(obs).flatten() # Flatten state into 1D array\n",
    "               \n",
    "        # Select and perform an action\n",
    "        a = actor_critic.select_action(s, \"exploration\")\n",
    "        # print(\"pred a: \", a) # Verify predicted action value\n",
    "               \n",
    "        # One Step\n",
    "        obs, r, done, info = PARAMS.env.step(a) # r: immediate reward, done: terminal state indicator\n",
    "        s1 = np.float32(obs).flatten() # Flatten state into 1D array\n",
    "        \n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.remember(s, a, r, s1)\n",
    "        \n",
    "        # Train the networks\n",
    "        actor_critic.train()\n",
    "        \n",
    "        episode_reward += r\n",
    "\n",
    "        if done:\n",
    "            s1 = None\n",
    "            # -------------------\n",
    "            # Log episode reward\n",
    "            # -------------------\n",
    "            log_file.write(str(episode_reward) + '\\n')\n",
    "            log_file.flush()\n",
    "            \n",
    "            if len(last_ten_episode_rewards) < 10:\n",
    "                last_ten_episode_rewards.append(episode_reward)\n",
    "                print(\"episode_i: \", episode_i, \"episode reward: \", episode_reward, \" Episode finished after {} timesteps\".format(t+1))\n",
    "            else:\n",
    "                last_ten_episode_rewards[last_ten_episode_rewards_i] = episode_reward\n",
    "                last_ten_episode_rewards_i += 1\n",
    "                last_ten_episode_rewards_i %= 10\n",
    "                print(\"episode_i: \", episode_i, \"episode reward: \", episode_reward, \" mean reward: \", np.mean(last_ten_episode_rewards), \" Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "            \n",
    "    # -------------\n",
    "    # Save weights\n",
    "    # -------------\n",
    "    if episode_i % PARAMS.save_model_episode_interval == 0:\n",
    "        os.makedirs((PARAMS.saved_models_path), exist_ok=True)\n",
    "        actor_critic.save_models(episode_i)\n",
    "        \n",
    "    episode_i += 1\n",
    "    \n",
    "PARAMS.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oRca5h6VJJo"
   },
   "outputs": [],
   "source": [
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VwsrB_LVJJq",
    "outputId": "1f7d0ab4-1484-495f-caf8-b183e0791777"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check tensorboard now.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "#  Write log data for visualization in Tensorboard\n",
    "# -------------------------------------------------\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class DataLogger():\n",
    "    def __init__(self):\n",
    "        self.log_path = PARAMS.log_path\n",
    "        self.logdir = self.log_path[:-1] + '/runs/'\n",
    "        self.writer = SummaryWriter(logdir=self.logdir)\n",
    "        \n",
    "    def write2tb(self):\n",
    "        # Write log data into tensorboard\n",
    "        log_file = open((PARAMS.log_path + 'episode_reward.log'), 'r')\n",
    "        for i, reward in enumerate(log_file):\n",
    "            self.writer.add_scalar('episode_reward', float(reward), i)\n",
    "        \n",
    "\n",
    "data_logger = DataLogger()\n",
    "data_logger.write2tb()\n",
    "print(\"Check tensorboard now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "To8xOg2gVJJu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cao_Bo_112130213_hct_actor_critic_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
